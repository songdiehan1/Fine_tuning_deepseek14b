# ğŸš€ DeepSeek-14B Fine-Tuning Project
This project aims to fine-tune DeepSeek-R1-Distill-Qwen-14B using Zhihu and Xiaohongshu datasets separately. The fine-tuning is conducted using the LoRA method with the Unsloth framework, optimized for Colab + A100 with BF16 support. The repository includes scripts for model training, evaluation, and inference.

## ğŸ“ Project Structure

## ğŸš€ Features
```bash
DeepSeek-Finetune/
â”‚â”€â”€ datasets/                        # Dataset folder (JSON data + preprocessing scripts)
â”‚   â”œâ”€â”€ raw/                         
â”‚   â”‚   â”œâ”€â”€ zhihu_data_without_clean.json         
â”‚   â”‚   â”œâ”€â”€ xhs_data_without_clean.json            
â”‚   â”œâ”€â”€ data/                         
â”‚   â”‚   â”œâ”€â”€ zhihu_data.json          
â”‚   â”‚   â”œâ”€â”€ xhs_data.json            
â”‚   â”œâ”€â”€ preprocess_zhihu.py          
â”‚   â”œâ”€â”€ preprocess_xhs.py    
â”‚â”€â”€ models/                          # Fine-tuned models
â”‚   â”œâ”€â”€ zhihu_lora_adapter/          
â”‚   â”œâ”€â”€ xhs_lora_adapter/    
â”‚â”€â”€ training/                        
â”‚   â”œâ”€â”€ train_zhihu.py               
â”‚   â”œâ”€â”€ train_xhs.py         
â”‚â”€â”€ inference/                       
â”‚   â”œâ”€â”€ inference_zhihu.py           
â”‚   â”œâ”€â”€ inference_xhs.py
â”‚â”€â”€ scripts/                       
â”‚   â”œâ”€â”€zhihu_colab_demo.ipynb            
â”‚   â”œâ”€â”€xhs_colab_demo.ipynb
â”‚â”€â”€ wandb/                       
â”‚   â”œâ”€â”€zhihu_train_loss.png           
â”‚   â”œâ”€â”€xhs_train_loss.png                   
â”‚â”€â”€ requirements.txt                 
â”‚â”€â”€ README.md                        
```








ğŸ”¥ Ready to fine-tune DeepSeek? Letâ€™s go! ğŸš€
